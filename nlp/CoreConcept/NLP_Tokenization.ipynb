{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnaNiwc3bM_y",
        "outputId": "52e0d670-6cb7-4b69-dc8d-c72bafef26d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('punkt_tab')           # Tokenizer model\n",
        "nltk.download('stopwords')       # Stop words list\n",
        "nltk.download('averaged_perceptron_tagger_eng')  # POS tagger model\n",
        "nltk.download('wordnet')         # Lemmatizer dictionary\n",
        "nltk.download('omw-1.4')         # Lemmatizer corpora"
      ],
      "metadata": {
        "id": "kt53hlA-cNAq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e09ec6ae-892e-4e26-8dfa-9a65102d421a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.rmtree('/root/nltk_data/tokenizers/punkt', ignore_errors=True)"
      ],
      "metadata": {
        "id": "hsSc_sP8IIQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sententce = \"The stripped bats are hanging on best for test\"\n",
        "word_tokens = word_tokenize(sententce)\n",
        "print(word_tokens)"
      ],
      "metadata": {
        "id": "cqkTlalddc_z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85ef7340-7f52-44f0-b267-b7673645e959"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'stripped', 'bats', 'are', 'hanging', 'on', 'best', 'for', 'test']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"There are lots of tress but the tress with green leaves are very few\"\n",
        "word_tokens= word_tokenize(sentence)\n",
        "print(word_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BF-tqylpJeq6",
        "outputId": "8f94524f-5dd0-4701-d23b-6aac6c3df7d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['There', 'are', 'lots', 'of', 'tress', 'but', 'the', 'tress', 'with', 'green', 'leaves', 'are', 'very', 'few']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Sure! Let's go through each core NLP concept with explanations and Python code examples using NLTK and spaCy (two widely used NLP libraries). We’ll use a sample sentence:\"\n",
        "sent_tokesn = sent_tokenize(sentence)\n",
        "print(sent_tokesn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PI-JaGT7JxOw",
        "outputId": "ab2fb833-1ca2-4861-d2f7-2332e47beeb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sure!', \"Let's go through each core NLP concept with explanations and Python code examples using NLTK and spaCy (two widely used NLP libraries).\", 'We’ll use a sample sentence:']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Sentence tokenization is the process of dividing a text document or a large block of text into individual sentences called tokens. It is a fundamental step in Natural Language Processing (NLP) that allows algorithms to handle and analyze text at the sentence level rather than as a continuous stream. By breaking text into sentences, NLP models can perform tasks like sentiment analysis, summarization, or translation more effectively on each distinct sentence.\"\n",
        "sent_tokesn = sent_tokenize(sentence)\n",
        "print(sent_tokesn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_LVn9XoKVZk",
        "outputId": "8bcec86d-e683-4573-c3ec-992d5eb864b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sentence tokenization is the process of dividing a text document or a large block of text into individual sentences called tokens.', 'It is a fundamental step in Natural Language Processing (NLP) that allows algorithms to handle and analyze text at the sentence level rather than as a continuous stream.', 'By breaking text into sentences, NLP models can perform tasks like sentiment analysis, summarization, or translation more effectively on each distinct sentence.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDFAY2jCKVtl",
        "outputId": "747f95a0-46f0-4c88-804f-80423507dcb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "rqGCX_SZM6Zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"lammatized 'bats':\", lemmatizer.lemmatize(\"bats\"))\n",
        "print(\"lammatized 'hanging':\", lemmatizer.lemmatize(\"hanging\", pos=wordnet.VERB))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewHu3K-2M_4Z",
        "outputId": "54fe55f4-0041-406c-ba6d-197e533aed8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lammatized 'bats': bat\n",
            "lammatized 'hanging': hang\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"lemmatize 'sentences':\", lemmatizer.lemmatize(\"sentences\"))\n",
        "print(\"lemmatze 'dividing':\", lemmatizer.lemmatize(\"dividing\", pos=wordnet.VERB))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTl3SZ3yNiAA",
        "outputId": "e6ac26c0-bd97-42a0-c41b-7a4dae87f371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lemmatize 'sentences': sentence\n",
            "lemmatze 'dividing': divide\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "CJ9sFWhzP0EC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words= [word for word in word_tokens if word.lower() not in stop_words]\n",
        "print(filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mj8ty6KWP9kP",
        "outputId": "048dfa58-f1a8-418f-dc5b-0a9ae82439a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['lots', 'tress', 'tress', 'green', 'leaves']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "pos_tages = nltk.pos_tag(word_tokens)\n",
        "print(pos_tages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwDMoVC5QaFj",
        "outputId": "30693c89-824b-4779-c8a8-cf4ea17aa423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('There', 'EX'), ('are', 'VBP'), ('lots', 'NNS'), ('of', 'IN'), ('tress', 'NN'), ('but', 'CC'), ('the', 'DT'), ('tress', 'NN'), ('with', 'IN'), ('green', 'JJ'), ('leaves', 'NNS'), ('are', 'VBP'), ('very', 'RB'), ('few', 'JJ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "yKzkTv_ya7DD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(sentence)"
      ],
      "metadata": {
        "id": "nNm5QSCdbCI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tokens:\", [token.text for token in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h02_zz1RbB7o",
        "outputId": "a46434b3-cf78-4b1d-ff84-43423871dd6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['Sentence', 'tokenization', 'is', 'the', 'process', 'of', 'dividing', 'a', 'text', 'document', 'or', 'a', 'large', 'block', 'of', 'text', 'into', 'individual', 'sentences', 'called', 'tokens', '.', 'It', 'is', 'a', 'fundamental', 'step', 'in', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'that', 'allows', 'algorithms', 'to', 'handle', 'and', 'analyze', 'text', 'at', 'the', 'sentence', 'level', 'rather', 'than', 'as', 'a', 'continuous', 'stream', '.', 'By', 'breaking', 'text', 'into', 'sentences', ',', 'NLP', 'models', 'can', 'perform', 'tasks', 'like', 'sentiment', 'analysis', ',', 'summarization', ',', 'or', 'translation', 'more', 'effectively', 'on', 'each', 'distinct', 'sentence', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"LemmaS:\", [token.lemma_ for token in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8K2HKOLbePn",
        "outputId": "75531274-a903-4b9d-968b-1e9f552ad728"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LemmaS: ['sentence', 'tokenization', 'be', 'the', 'process', 'of', 'divide', 'a', 'text', 'document', 'or', 'a', 'large', 'block', 'of', 'text', 'into', 'individual', 'sentence', 'call', 'token', '.', 'it', 'be', 'a', 'fundamental', 'step', 'in', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'that', 'allow', 'algorithm', 'to', 'handle', 'and', 'analyze', 'text', 'at', 'the', 'sentence', 'level', 'rather', 'than', 'as', 'a', 'continuous', 'stream', '.', 'by', 'break', 'text', 'into', 'sentence', ',', 'NLP', 'model', 'can', 'perform', 'task', 'like', 'sentiment', 'analysis', ',', 'summarization', ',', 'or', 'translation', 'more', 'effectively', 'on', 'each', 'distinct', 'sentence', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"POS Tage:\", [(token.text,token.pos_) for token in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_3aBhffbugG",
        "outputId": "1b8d4a50-0e5c-42ff-964d-77c6d3d3c1c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tage: [('Sentence', 'NOUN'), ('tokenization', 'NOUN'), ('is', 'AUX'), ('the', 'DET'), ('process', 'NOUN'), ('of', 'ADP'), ('dividing', 'VERB'), ('a', 'DET'), ('text', 'NOUN'), ('document', 'NOUN'), ('or', 'CCONJ'), ('a', 'DET'), ('large', 'ADJ'), ('block', 'NOUN'), ('of', 'ADP'), ('text', 'NOUN'), ('into', 'ADP'), ('individual', 'ADJ'), ('sentences', 'NOUN'), ('called', 'VERB'), ('tokens', 'NOUN'), ('.', 'PUNCT'), ('It', 'PRON'), ('is', 'AUX'), ('a', 'DET'), ('fundamental', 'ADJ'), ('step', 'NOUN'), ('in', 'ADP'), ('Natural', 'PROPN'), ('Language', 'PROPN'), ('Processing', 'PROPN'), ('(', 'PUNCT'), ('NLP', 'PROPN'), (')', 'PUNCT'), ('that', 'PRON'), ('allows', 'VERB'), ('algorithms', 'NOUN'), ('to', 'PART'), ('handle', 'VERB'), ('and', 'CCONJ'), ('analyze', 'VERB'), ('text', 'NOUN'), ('at', 'ADP'), ('the', 'DET'), ('sentence', 'NOUN'), ('level', 'NOUN'), ('rather', 'ADV'), ('than', 'ADP'), ('as', 'ADP'), ('a', 'DET'), ('continuous', 'ADJ'), ('stream', 'NOUN'), ('.', 'PUNCT'), ('By', 'ADP'), ('breaking', 'VERB'), ('text', 'NOUN'), ('into', 'ADP'), ('sentences', 'NOUN'), (',', 'PUNCT'), ('NLP', 'PROPN'), ('models', 'NOUN'), ('can', 'AUX'), ('perform', 'VERB'), ('tasks', 'NOUN'), ('like', 'ADP'), ('sentiment', 'NOUN'), ('analysis', 'NOUN'), (',', 'PUNCT'), ('summarization', 'NOUN'), (',', 'PUNCT'), ('or', 'CCONJ'), ('translation', 'NOUN'), ('more', 'ADV'), ('effectively', 'ADV'), ('on', 'ADP'), ('each', 'DET'), ('distinct', 'ADJ'), ('sentence', 'NOUN'), ('.', 'PUNCT')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"stop words removal:\", [token.text for token in doc if not token.is_stop])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8O1ikLwFcHix",
        "outputId": "6c994a19-1337-4f02-b4ba-31894317512c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stop words removal: ['Sentence', 'tokenization', 'process', 'dividing', 'text', 'document', 'large', 'block', 'text', 'individual', 'sentences', 'called', 'tokens', '.', 'fundamental', 'step', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'allows', 'algorithms', 'handle', 'analyze', 'text', 'sentence', 'level', 'continuous', 'stream', '.', 'breaking', 'text', 'sentences', ',', 'NLP', 'models', 'perform', 'tasks', 'like', 'sentiment', 'analysis', ',', 'summarization', ',', 'translation', 'effectively', 'distinct', 'sentence', '.']\n"
          ]
        }
      ]
    }
  ]
}